\documentclass[12pt,letterpaper]{article}

\usepackage{ifpdf}
\usepackage{mla}

\begin{document}
\begin{mla}{Minsheng}{Liu}{Marissa Perino}{WCWP 10A}{03/02/16}
  {Big Data is Good for the Academia}

Big data analytics has become an increasingly popular problem-solving
approach. Is this a healthy phenomenon? In academic research, many
people have the concern that the nature of big data analytics---that it
is more about correlational descriptions than causal
explanations---would have negative impacts on human being's pursuit of
the explanation of the universe. Unfortunately, supporters of big data
sometimes ignore this type of counterarguments. In the book Big Data,
Viktor Mayer-Schönberger and Kenneth Cuvier concentrates on the
applications and ramifications of big data in industry rather than in
academia (Mayer-Schönberger et al. 153 -- 196). Cesar Hidalgo, a
professor at MIT, responded to critics of this sort by asking them to
see big data's immediate benefits instead of refuting them directly
(Hidalgo 150). Big data analytics does not necessarily hinder
researchers' pursuit of causal explanations. Results gained from big
data analytics are sometimes indispensable for explanatory theories, and
such techniques can be useful for scientists.

Big data analytics can directly facilitate existing research. For
example, this technology can simplify experiments in social science. In
his article \emph{Visible Man}, Singer mentioned that people behave more
rightly when they are ``liable to be observed'' (Singer 36). In his
description, researchers set up a complicated experiment, which took
lots of time and resources. However, with big data analytics,
researchers may simply collect user behavior data from a website and
compare differences between users that are logged in and those not. Such
an approach might be unrealistic at present due to complicated issues
such as privacy, but as similar practices continue to happen, eventually
it will be extremely cheap. After all, there is no experiment; there is
only an analysis of existing data.

Another way that big data analytics can make researchers' lives much
easier is best characterized by a simple example in math.
Mathematically, a function is a procedure that given some input yields
some output. When analyzing the behavior of a function, mathematicians
often plot the graph of that function, even though such a figure cannot
form any part of a formal proof. Why do mathematicians still do this?
They do because from these graphs mathematicians can have a general
understanding of functions in question so that they can pick the right
tools. It is interesting that in many cases, function graphs are too
complicated to draw by hand. Mathematicians have to use a computer to
draw graphs. How similar is this to big data! It is not hard to conceive
that today's social scientists will face data too complex to be analyzed
by hand. Social scientists need their function plotter---big data
analytics---to have some research directions before conducting any
manual analysis. Moreover, in the future social scientists might be able
to ask the machine to find interesting phenomena automatically and
conduct in-depth case studies afterward. For instance, researchers have
found ``the `data signature' of natural leaders'' (Peck 9), from which
psychologists might be able to discover something useful.

Nevertheless, there is a common though untenable opposition against big
data analytics: this technology will ``distract'' researchers from
explaining our world causally because it can only reveal correlational
phenomena (Hidalgo 149). The critics of big data analytics are wrong in
that they underestimate institutions of human society that protect
researchers from such distraction and that they falsely assume that such
analysis method is exclusive. Our existing social institutions can keep
researchers finding causal explanations. For example, peer reviews
during the paper publication process can prevent publishing articles
deviating too much from causal studies. Afraid of having one's paper
submission rejected, a researcher will avoid writing drafts lacking
convincing explanations as much as possible. A much more important
aspect of our society, however, is its strong preference for causal
explanations. This preference had already existed when Galileo and his
peers started modern science by using the quantitative method. In fact,
it can be traced back to Aristotle's formal study of logic. Stories of
those ancient thinkers are told over and over again to each generation.
Thus, it is impossible for us to give up our pursuit of causal
explanations.

The fact that even from a utilitarian perspective big data analytics is
beneficial for human's pursuit of explanatory theories is the other
reason those opponents of big data analytics are merely alarmists.
Because of these academic rules and our society's education as discussed
above, researchers are probably the ones who love causal explanations
most. When a researcher chooses to publish something that only reveals
correlations about a subject, that subject must be hard to research.
Such practice is not detrimental, as one should note that some progress
is better than no progress. If a researcher decides not to publish
anything until he or she can give a convincing theory, other researchers
will be discouraged from solving the problem and grants will likely to
fly away. From this perspective, applying big data analytics to a field
is beneficial for the field's long-term development. Moreover, as Gary
Marcus and Ernest Davis pointed out in their article, big data analytics
is a tool to complement other methods rather than an exclusive one
(Marcus et al. 154). Continue the above example. After the researcher
uses big data techniques to publish some correlational descriptions, he
or she might continue in-depth analysis---probably after gaining more
grants---to obtain some explanatory theory. In short, big data analytics
cannot pose a threat to our effort of explaining the world.

There is an even trickier counterargument: applying big data analytics
to traditional subjects of science yields many new areas, but any
research efforts in these fields are a waste of resources because they
do not help us understand the world better. To understand this
counterargument, one can look at the subject of natural language
processing. Natural language processing is a newly developed
interdisciplinary field developed from linguistics and computer science.
It heavily relies on big data technologies such us machine learning and
statistical methods (Mayer-Schönberger et al. 115). A particular example
is machine translation, which is currently done by feeding machines with
large corpus (Mayer-Schönberger et al. 85). Machine learning algorithms
can automatically figure out some enigmatic patterns underlying human
languages, possibly in the form of formulas involving millions of
variables. The result of this approach is fantastic. Despite its real
world application, a theoretical linguist would argue that such advance
is unhelpful for understanding how the brain works---the ultimate goal
of studying natural languages. Indeed, the models generated by machine
learning are meaningless to human beings because no one can understand
formulas involving so many variables. Moreover, when traditional
linguists finally give a satisfactory explanatory theory of our language
capabilities, no one will be interested in studying these old models
generated by big data analytics. In other words, once we make
theoretical breakthroughs in a traditional area, the subject
``generated'' from that field will become useless. Any research effort
in these ``generated'' fields, in the long term, is meaningless.

Nothing is further from the truth. The results obtained in fields
``generated'' by big data analytics---both experiences accumulated and
tools built---can be quite useful when researchers find explanatory
answers.

Everyone knows that it was Newton who discovered the gravity and who
explained the movements of stars in our solar system. Does this mean
that astronomical observations before Newton were useless? After all,
with Newton's discovery, we could predict how a planet would move and
recalculate orbit data that were observed by previous astronomers. The
answer is no. Newton needed these data to build a possible theory in the
first place. After that, he also needed other observations to verify his
theory. In a similar manner, current achievements in machine translation
could also be helpful when developing theories about our brains and
language capacities. Even though scientists might not be able to say the
exact meaning of each variable of a machine-learnt model, linguists can
still find some general patterns from those models. Then, based on
linguists' findings, cognitive scientists can narrow down their searches
for possible brain models. Another use of language machines trained by
large corpus (footnote: a collection of written or spoken material in
machine-readable form, assembled for the purpose of studying linguistic
structures, frequencies, etc. ``New Oxford American Dictionary'') is to
serve as evaluation tools for developing models in cognitive science.
Instead of setting up an expensive experiment with real human beings,
scientists can first verify their theories using machines. In short,
these ``generated'' fields, though seemingly \emph{ad hoc}, are
indispensable in finding causal explanations. Without efforts in these
areas, finding a convincing theory would be much harder, if not
impossible.

Big data is a game-changer. In the past, we could only process a small
amount of data which can only reflect some ``local'' rules. Think an
analogy. Ancient dwellers of San Diego would not know what snow
was---they never saw them. That was small data. Big data is a satellite
that allows us to see the weather of any place on this planet.
Abstractly, if we know something's properties ``globally'' and if given
any parameters we can make accurate predictions of that thing, does it
matter whether we understand the underlying rules of that thing?

It is tempting to answer no. However, no physicist in 1904 could realize
the ramification of the mystery feature of light speed. Similarly, one
would never know the implication of an explanatory theory. This is the
reason we cannot give up our pursuit of explaining the world. Big data
analytics appears to pose a threat to this principle, but in fact, it
strengthens the principle. Researchers are the least group of people
satisfied with merely describing a phenomenon, and when they do so, they
must aim for a greater good. The whole society, as well as institutions
in the academia, also prevent human beings from discontinuing our desire
for explanations. Though there are some \emph{ad hoc} academic fields
whose solely purposes are their real world applications, they are in
fact indispensable to our future understanding of the world. Moreover,
big data analytics can bring immediate benefits to nowadays research.
Consider the significant benefits brought by big data analytics, I would
propose to use big data analytics as default strategy in most fields.
➜  wi16-warren-writing git:(master) ✗ pandoc 3F.md -t latex
Big data analytics has become an increasingly popular problem-solving
approach. Is this a healthy phenomenon? In academic research, many
people have the concern that the nature of big data analytics---that it
is more about correlational descriptions than causal
explanations---would have negative impacts on human being's pursuit of
the explanation of the universe. Unfortunately, supporters of big data
sometimes ignore this type of counterarguments. In the book Big Data,
Viktor Mayer-Schönberger and Kenneth Cuvier concentrates on the
applications and ramifications of big data in industry rather than in
academia (Mayer-Schönberger et al. 153 -- 196). Cesar Hidalgo, a
professor at MIT, responded to critics of this sort by asking them to
see big data's immediate benefits instead of refuting them directly
(Hidalgo 150). Big data analytics does not necessarily hinder
researchers' pursuit of causal explanations. Results gained from big
data analytics are sometimes indispensable for explanatory theories, and
such techniques can be useful for scientists.

Big data analytics can directly facilitate existing research. For
example, this technology can simplify experiments in social science. In
his article \emph{Visible Man}, Singer mentioned that people behave more
rightly when they are ``liable to be observed'' (Singer 36). In his
description, researchers set up a complicated experiment, which took
lots of time and resources. However, with big data analytics,
researchers may simply collect user behavior data from a website and
compare differences between users that are logged in and those not. Such
an approach might be unrealistic at present due to complicated issues
such as privacy, but as similar practices continue to happen, eventually
it will be extremely cheap. After all, there is no experiment; there is
only an analysis of existing data.

Another way that big data analytics can make researchers' lives much
easier is best characterized by a simple example in math.
Mathematically, a function is a procedure that given some input yields
some output. When analyzing the behavior of a function, mathematicians
often plot the graph of that function, even though such a figure cannot
form any part of a formal proof. Why do mathematicians still do this?
They do because from these graphs mathematicians can have a general
understanding of functions in question so that they can pick the right
tools. It is interesting that in many cases, function graphs are too
complicated to draw by hand. Mathematicians have to use a computer to
draw graphs. How similar is this to big data! It is not hard to conceive
that today's social scientists will face data too complex to be analyzed
by hand. Social scientists need their function plotter---big data
analytics---to have some research directions before conducting any
manual analysis. Moreover, in the future social scientists might be able
to ask the machine to find interesting phenomena automatically and
conduct in-depth case studies afterward. For instance, researchers have
found ``the `data signature' of natural leaders'' (Peck 9), from which
psychologists might be able to discover something useful.

Nevertheless, there is a common though untenable opposition against big
data analytics: this technology will ``distract'' researchers from
explaining our world causally because it can only reveal correlational
phenomena (Hidalgo 149). The critics of big data analytics are wrong in
that they underestimate institutions of human society that protect
researchers from such distraction and that they falsely assume that such
analysis method is exclusive. Our existing social institutions can keep
researchers finding causal explanations. For example, peer reviews
during the paper publication process can prevent publishing articles
deviating too much from causal studies. Afraid of having one's paper
submission rejected, a researcher will avoid writing drafts lacking
convincing explanations as much as possible. A much more important
aspect of our society, however, is its strong preference for causal
explanations. This preference had already existed when Galileo and his
peers started modern science by using the quantitative method. In fact,
it can be traced back to Aristotle's formal study of logic. Stories of
those ancient thinkers are told over and over again to each generation.
Thus, it is impossible for us to give up our pursuit of causal
explanations.

The fact that even from a utilitarian perspective big data analytics is
beneficial for human's pursuit of explanatory theories is the other
reason those opponents of big data analytics are merely alarmists.
Because of these academic rules and our society's education as discussed
above, researchers are probably the ones who love causal explanations
most. When a researcher chooses to publish something that only reveals
correlations about a subject, that subject must be hard to research.
Such practice is not detrimental, as one should note that some progress
is better than no progress. If a researcher decides not to publish
anything until he or she can give a convincing theory, other researchers
will be discouraged from solving the problem and grants will likely to
fly away. From this perspective, applying big data analytics to a field
is beneficial for the field's long-term development. Moreover, as Gary
Marcus and Ernest Davis pointed out in their article, big data analytics
is a tool to complement other methods rather than an exclusive one
(Marcus et al. 154). Continue the above example. After the researcher
uses big data techniques to publish some correlational descriptions, he
or she might continue in-depth analysis---probably after gaining more
grants---to obtain some explanatory theory. In short, big data analytics
cannot pose a threat to our effort of explaining the world.

There is an even trickier counterargument: applying big data analytics
to traditional subjects of science yields many new areas, but any
research efforts in these fields are a waste of resources because they
do not help us understand the world better. To understand this
counterargument, one can look at the subject of natural language
processing. Natural language processing is a newly developed
interdisciplinary field developed from linguistics and computer science.
It heavily relies on big data technologies such us machine learning and
statistical methods (Mayer-Schönberger et al. 115). A particular example
is machine translation, which is currently done by feeding machines with
large corpus (Mayer-Schönberger et al. 85). Machine learning algorithms
can automatically figure out some enigmatic patterns underlying human
languages, possibly in the form of formulas involving millions of
variables. The result of this approach is fantastic. Despite its real
world application, a theoretical linguist would argue that such advance
is unhelpful for understanding how the brain works---the ultimate goal
of studying natural languages. Indeed, the models generated by machine
learning are meaningless to human beings because no one can understand
formulas involving so many variables. Moreover, when traditional
linguists finally give a satisfactory explanatory theory of our language
capabilities, no one will be interested in studying these old models
generated by big data analytics. In other words, once we make
theoretical breakthroughs in a traditional area, the subject
``generated'' from that field will become useless. Any research effort
in these ``generated'' fields, in the long term, is meaningless.

Nothing is further from the truth. The results obtained in fields
``generated'' by big data analytics---both experiences accumulated and
tools built---can be quite useful when researchers find explanatory
answers.

Everyone knows that it was Newton who discovered the gravity and who
explained the movements of stars in our solar system. Does this mean
that astronomical observations before Newton were useless? After all,
with Newton's discovery, we could predict how a planet would move and
recalculate orbit data that were observed by previous astronomers. The
answer is no. Newton needed these data to build a possible theory in the
first place. After that, he also needed other observations to verify his
theory. In a similar manner, current achievements in machine translation
could also be helpful when developing theories about our brains and
language capacities. Even though scientists might not be able to say the
exact meaning of each variable of a machine-learnt model, linguists can
still find some general patterns from those models. Then, based on
linguists' findings, cognitive scientists can narrow down their searches
for possible brain models. Another use of language machines trained by
large corpus\footnote{A collection of written or spoken material in
machine-readable form, assembled for the purpose of studying linguistic
structures, frequencies, etc. from \textit{New Oxford American Dictionary.}. is to
serve as evaluation tools for developing models in cognitive science.
Instead of setting up an expensive experiment with real human beings,
scientists can first verify their theories using machines. In short,
these ``generated'' fields, though seemingly \emph{ad hoc}, are
indispensable in finding causal explanations. Without efforts in these
areas, finding a convincing theory would be much harder, if not
impossible.

Big data is a game-changer. In the past, we could only process a small
amount of data which can only reflect some ``local'' rules. Think an
analogy. Ancient dwellers of San Diego would not know what snow
was---they never saw them. That was small data. Big data is a satellite
that allows us to see the weather of any place on this planet.
Abstractly, if we know something's properties ``globally'' and if given
any parameters we can make accurate predictions of that thing, does it
matter whether we understand the underlying rules of that thing?

It is tempting to answer no. However, no physicist in 1904 could realize
the ramification of the mystery feature of light speed. Similarly, one
would never know the implication of an explanatory theory. This is the
reason we cannot give up our pursuit of explaining the world. Big data
analytics appears to pose a threat to this principle, but in fact, it
strengthens the principle. Researchers are the least group of people
satisfied with merely describing a phenomenon, and when they do so, they
must aim for a greater good. The whole society, as well as institutions
in the academia, also prevent human beings from discontinuing our desire
for explanations. Though there are some \emph{ad hoc} academic fields
whose solely purposes are their real world applications, they are in
fact indispensable to our future understanding of the world. Moreover,
big data analytics can bring immediate benefits to nowadays research.
With the significant benefits brought by big data analytics in mind, it
is reasonable to propose to use big data analytics more frequently, not
less.


\subsection*{Works Cited}
\bibent Mayer-Schönberger, Viktor and Cukier, Kenneth. ``Big Data.'' 2013. Print.

\bibent Hidalgo, Cesar, ``Saving Big Data from Big Mouths.'' \textit{Scientific American} 2014. Web.

\bibent Marcus, Gar and Ernest Davis. ``Eight (No, Nine!) Problems With Big Data.'' \textit{New York Times}. 2014. Web.

\bibent Singer, Peter. ``Visible Man: Ethics in a World without Secrets.'' \textit{Harper's Magazine}. Spept. 2011: 31-36. Print

\bibent Peck, Don. ``They're Watching You at Work.'' \textit{The Atlantic}. 2013. Web.

\end{mla}
\end{document}
